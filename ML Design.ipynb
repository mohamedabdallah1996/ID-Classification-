{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think until now there are no big limitations with OpenCV in image processing. `OpenCV` is a powerful tool for image processing. it's fast, portable and more efficient. Due to its source-code in C/C++, any machine capable of running C/C++, can have OpenCV running as well. <br>\n",
    "it's also free, simple to learn, support many languages and you can make more time and memory optimizations with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Machine Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can build a simple deep learning architecture using `Keras`. `Keras` is a deep learning framework easy to learn, easy to use\n",
    "and can build complex deep learning architecture by a few lines of code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can build simple model with keras. stack some convolution layers over pooling layers, then one or two dense layers at the end. finally add a dense layer with the number of your classes as the output of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size = (3, 3), activation='relu', input_shape=(500, 300, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# output 2 classes (real, printed)\n",
    "model.add(Dense(2, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we chose a small `filter size` in order to captures smaller and complex features in the image so that the amount of information extracted will be vast, maybe useful in later layers. \n",
    "also we are increasing `number of filters` in order to detect more features with advanced layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pooling Layer` has 2 advantages here:\n",
    "> 1. Progressive reduction of the spatial size of feature map as we move from one convolution layer to the next, thus helping to reduce number of parameters\n",
    "> 2. Progressively identifying important features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we add `Batch Normalization` layer. `Batch normalization` is a technique for training deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the activation function, we used `relu`. `ReLU` is important here because it does not saturate so the gradient is always high (equal to 1) if the neuron activates. Compare this to `sigmoid` or `tanh`. both of which saturate (if the input is very high or very low, the gradient is very very small)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amount Of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of training data will depend on various criterion:-\n",
    "\n",
    "> 1. The type of network/model used.\n",
    "> 2. The parameters of the model.\n",
    "> 3. Preprocessing of the dataset.\n",
    "> 4. Representation of the dataset.\n",
    "> 5. Actual dimensionality of the input.\n",
    "\n",
    "if you will build the model from scratch, you will need large amount of data to generalize well. But you can use transfer learning if there is a pretrained model similar to your problem, this will simplify your task. you just freeze the weights of the pretrained model on the earlier layers and train the last layers with your few images.\n",
    "<br><br>\n",
    "in our case we have image classification model. we trained the model with 800 images and it reached a quite good accuracy. But for sure with more data the performance of the model will be improved and it will be able to generalize.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many data preprocessing techniques with convolutional neural network. But these are the most important techniques:\n",
    "\n",
    "> Mean normalization<br>\n",
    "Mean normalization is just removing the mean from each observation. it has the effect of centering the data around 0\n",
    "\n",
    "> Standarization <br>\n",
    "Standardization is used to put all features on the same scale. Each zero-centered dimension is divided by its standard deviation.\n",
    "\n",
    "> Resizing <br>\n",
    "We should resize all training images because the model expects to have all images with the sampe shape.\n",
    "\n",
    "> Dimensionality Reduction <br>\n",
    "We may use some dimensionality reduction techniques like: PCA or LDA to reduce the dimensionality of the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Accuray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think with CNN architecture the model will get more accuracy because convolution layers will detect complex features with its filters and accomulate these features over the convolution layers to learn more and more complex features. But we should notice that we don't have very large amount of data so if we built a very powerful CNN model with many layers, the model may overfit the data and get bad accuracy with the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jenv",
   "language": "python",
   "name": "jenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
